{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_examples_03_language_models.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5/1c+Jb2EVTHzRjDqH2V6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marekrei/ml-examples/blob/main/ML_examples_03_language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Examples - 03 - Language Models"
      ],
      "metadata": {
        "id": "xhkA0Mg7ExR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text classification using a BERT encoder model"
      ],
      "metadata": {
        "id": "t0B6brB5FLZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section4.ipynb\n",
        "\n",
        "Training the BERT model for binary sentiment detection, using the SST2 dataset."
      ],
      "metadata": {
        "id": "iWy24yN3Ex7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text classification example with BERT\n",
        "# Created by Marek Rei\n",
        "# Based on https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section4.ipynb\n",
        "# Training the model for binary sentiment detection, using the SST2 dataset.\n",
        "\n",
        "# Some settings\n",
        "# Which pre-trained model to use.\n",
        "# See https://huggingface.co/models for options.\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "\n",
        "# How much training data to use.\n",
        "# 1.0 uses the whole training set but it can take a bit of time to train.\n",
        "train_data_sample_ratio = 0.1\n",
        "\n",
        "# Example sentence to use\n",
        "# We print out predictions for this sentence before and after training\n",
        "example_sentence = \"this was by far the best movie of the year\""
      ],
      "metadata": {
        "id": "5uwVDLHBtne4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTVWIb0qh3jT"
      },
      "outputs": [],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries\n",
        "import torch\n",
        "import evaluate\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from transformers import AdamW\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import get_scheduler\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "dne_D0C1m9hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking whether you are running on CPU or GPU.\n",
        "# If the output here says \"cuda\" then it's running on GPU. Otherwise it's probably CPU.\n",
        "# In order to run your code in Colab on the GPU, go to Edit -> Notebook settings -> Hardware accelerator and set it to \"GPU\".\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "e0TR4shQldnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the pretrained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Nn2BYjRlo1HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
        "raw_datasets.cleanup_cache_files()\n",
        "\n",
        "# Using only a sample of the training data if needed\n",
        "\n",
        "if train_data_sample_ratio < 1.0:\n",
        "    num_training_examples = int(train_data_sample_ratio*len(raw_datasets[\"train\"]))\n",
        "    raw_datasets[\"train\"] = load_dataset(\"glue\", \"sst2\", split='train[:'+str(num_training_examples)+']')\n",
        "\n",
        "# Perform tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence\"], truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "# Need to remove columns that the model won't know\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "tokenized_datasets[\"train\"].column_names\n",
        "\n",
        "# DataCollatorWithPadding constructs batches that are padded to the length of the longest sentence in the batch\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "bN573l4BjMvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing out the shapes in one batch\n",
        "example_batch = None\n",
        "for batch in train_dataloader:\n",
        "    example_batch = batch\n",
        "    break\n",
        "\n",
        "print({k: v.shape for k, v in example_batch.items()})\n",
        "\n",
        "\n",
        "# Then printing out the loss, output shape and output values from one batch.\n",
        "outputs = model(**example_batch.to(device))\n",
        "print(\"output.loss: \", outputs.loss)\n",
        "print(\"output.logits.shape: \", outputs.logits.shape)\n",
        "print(\"output.logits: \", outputs.logits)\n",
        "\n",
        "# Generating predictions for an example sentence.\n",
        "# Haven't trained the model yet so these will be random.\n",
        "def print_example_predictions(example_sentence, example_model):\n",
        "    _e = tokenize_function({\"sentence\": example_sentence})\n",
        "    _k = {k: torch.tensor([_e[k]]).to(device) for k in _e}\n",
        "    model.eval()\n",
        "    example_outputs = model(**_k)\n",
        "    example_logits = example_outputs.logits.cpu().detach().numpy()\n",
        "    example_probabilities = torch.nn.functional.softmax(example_outputs.logits, dim=1).cpu().detach().numpy()\n",
        "    print(example_probabilities)\n",
        "    print(\"Example sentence: \", example_sentence)\n",
        "    print(\"Predicted logits: \", example_logits)\n",
        "    print(\"Predicted probabilities: \", example_probabilities)\n",
        "    print(\"Prediction: \", \"negative\" if example_probabilities[0][0] > example_probabilities[0][1] else \"positive\")\n",
        "\n",
        "print_example_predictions(example_sentence, model)\n"
      ],
      "metadata": {
        "id": "DK1uI7JaklA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up model training for fine-tuning\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "-CnXmKlnkvfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the model to training mode\n",
        "model.train()\n",
        "\n",
        "# Running the training\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ],
      "metadata": {
        "id": "-e2hvHUpl3oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Running evaluation\n",
        "metric = evaluate.load(\"glue\", \"sst2\")\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "print(metric.compute())"
      ],
      "metadata": {
        "id": "silkvQPEmD7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting predictions for the example sentence again, now that we have trained the model\n",
        "print_example_predictions(example_sentence, model)"
      ],
      "metadata": {
        "id": "pKoQX-y_fK02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating output from a language model"
      ],
      "metadata": {
        "id": "P_PldT-3FKGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Define the model name\n",
        "MODEL_NAME = \"tiiuae/falcon-7b-instruct\"  # Replace with a smaller model if needed\n",
        "\n",
        "# Load the tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Ensure pad token is set (some models might not have one by default)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the model\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",  # Automatically maps to available GPU if present\n",
        "    torch_dtype=torch.float16  # Use float16 to save memory\n",
        ")\n",
        "\n",
        "# Define the example input\n",
        "example_input = \"Explain the importance of renewable energy in mitigating climate change.\"\n",
        "\n",
        "# Tokenize the input with padding and attention mask\n",
        "print(\"Tokenizing input...\")\n",
        "inputs = tokenizer(\n",
        "    example_input,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,  # Adds padding if necessary\n",
        "    truncation=True,  # Truncates input if it exceeds model's max length\n",
        "    max_length=512  # Adjust based on your model's max input length\n",
        ")\n",
        "\n",
        "# Move inputs to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Generate output\n",
        "print(\"Generating output...\")\n",
        "generation_output = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],  # Pass the attention mask\n",
        "    max_length=200,  # Adjust the length as needed\n",
        "    temperature=0.7,  # Control randomness in the output\n",
        "    top_p=0.9,  # Use nucleus sampling\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.pad_token_id  # Explicitly set the pad token ID\n",
        ")\n",
        "\n",
        "# Decode the output\n",
        "output_text = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the output\n",
        "print(\"\\nGenerated Output:\")\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "id": "K-s6izGRK8qC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}